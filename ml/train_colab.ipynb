{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† Psychiatric Disorder Detection - Model Training\n",
                "\n",
                "This notebook trains ML models for psychiatric disorder severity classification using the DASS-42 questionnaire.\n",
                "\n",
                "**Pipeline:**\n",
                "1. Download DASS-42 dataset from Kaggle\n",
                "2. Preprocess and select 30 RFE-identified features\n",
                "3. Apply feature scaling (StandardScaler)\n",
                "4. Train 4 classifiers (LogReg, RF, SVM, GB)\n",
                "5. Compare using comprehensive metrics\n",
                "6. Save best model + scaler for deployment\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîó Step 0: Mount Google Drive (Optional)\n",
                "\n",
                "**Run this if using VS Code + Colab extension.** Skip if using browser Colab."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "DRIVE_OUTPUT = '/content/drive/MyDrive/PDD_Models'\n",
                "\n",
                "import os\n",
                "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
                "print(f\"‚úÖ Drive mounted. Output: {DRIVE_OUTPUT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Step 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install kagglehub -q"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìö Step 2: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import joblib\n",
                "\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    confusion_matrix, roc_auc_score\n",
                ")\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "RANDOM_STATE = 42\n",
                "np.random.seed(RANDOM_STATE)\n",
                "\n",
                "print(\"‚úÖ Libraries imported!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚öôÔ∏è Step 3: Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SELECTED_FEATURES = [\n",
                "    'Q1A', 'Q3A', 'Q4A', 'Q5A', 'Q7A', 'Q8A', 'Q9A', 'Q10A', 'Q11A', 'Q12A',\n",
                "    'Q13A', 'Q16A', 'Q17A', 'Q20A', 'Q21A', 'Q22A', 'Q24A', 'Q26A', 'Q27A',\n",
                "    'Q28A', 'Q29A', 'Q30A', 'Q32A', 'Q33A', 'Q34A', 'Q36A', 'Q38A', 'Q39A',\n",
                "    'Q40A', 'Q41A'\n",
                "]\n",
                "\n",
                "CLASS_LABELS = {0: \"None\", 1: \"Mild\", 2: \"Moderate\", 3: \"Severe\"}\n",
                "TEST_SIZE = 0.2\n",
                "\n",
                "print(f\"Features: {len(SELECTED_FEATURES)}, Classes: {len(CLASS_LABELS)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì• Step 4: Download Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import kagglehub\n",
                "\n",
                "path = kagglehub.dataset_download(\"lucasgreenwell/depression-anxiety-stress-scales-responses\")\n",
                "DATA_PATH = list(Path(path).glob(\"*.csv\"))[0]\n",
                "print(f\"Dataset: {DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîÑ Step 5: Load and Preprocess Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(DATA_PATH, sep='\\t')\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "\n",
                "available_features = [f for f in SELECTED_FEATURES if f in df.columns]\n",
                "X = df[available_features].copy()\n",
                "\n",
                "# Create target from total DASS score (quartile-based)\n",
                "q_cols = [col for col in df.columns if col.endswith('A') and col.startswith('Q')]\n",
                "total_score = df[q_cols].sum(axis=1)\n",
                "\n",
                "y = pd.cut(total_score, \n",
                "           bins=[0, total_score.quantile(0.25), total_score.quantile(0.50),\n",
                "                 total_score.quantile(0.75), total_score.max() + 1],\n",
                "           labels=[0, 1, 2, 3], include_lowest=True)\n",
                "\n",
                "X = X.fillna(X.median())\n",
                "\n",
                "# Handle NaN in target\n",
                "if y.isna().any():\n",
                "    print(f\"‚ö†Ô∏è Dropping {y.isna().sum()} NaN targets\")\n",
                "    mask = ~y.isna()\n",
                "    X, y = X[mask], y[mask]\n",
                "\n",
                "y = y.astype(int)\n",
                "print(f\"Features: {X.shape}, Target: {y.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Step 6: Class Distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class_counts = y.value_counts().sort_index()\n",
                "for cls, count in class_counts.items():\n",
                "    print(f\"  {CLASS_LABELS[cls]}: {count:,} ({count/len(y)*100:.1f}%)\")\n",
                "\n",
                "plt.figure(figsize=(8, 5))\n",
                "plt.bar(range(4), class_counts.values, color=['#22c55e', '#eab308', '#f97316', '#ef4444'])\n",
                "plt.xticks(range(4), CLASS_LABELS.values())\n",
                "plt.title('Class Distribution')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚úÇÔ∏è Step 7: Train/Test Split + Scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
                ")\n",
                "\n",
                "# Apply StandardScaler (fit on train only)\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"Train: {X_train.shape[0]}, Test: {X_test.shape[0]}\")\n",
                "print(\"‚úÖ Scaler fitted\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ü§ñ Step 8: Train and Compare Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
                "    'SVM': SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE),\n",
                "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
                "}\n",
                "\n",
                "results = {}\n",
                "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"\\n--- {name} ---\")\n",
                "    model.fit(X_train_scaled, y_train)\n",
                "    y_pred = model.predict(X_test_scaled)\n",
                "    y_proba = model.predict_proba(X_test_scaled)\n",
                "    \n",
                "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
                "    \n",
                "    try:\n",
                "        roc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='weighted')\n",
                "    except (ValueError, TypeError):\n",
                "        roc = None\n",
                "    \n",
                "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='f1_weighted')\n",
                "    \n",
                "    results[name] = {\n",
                "        'accuracy': accuracy_score(y_test, y_pred),\n",
                "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
                "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
                "        'f1_weighted': f1,\n",
                "        'f1_macro': f1_score(y_test, y_pred, average='macro'),\n",
                "        'roc_auc': roc,\n",
                "        'cv_mean': cv_scores.mean(),\n",
                "        'cv_std': cv_scores.std(),\n",
                "        'model': model,\n",
                "        'y_pred': y_pred\n",
                "    }\n",
                "    \n",
                "    roc_str = f\"{roc:.4f}\" if roc else \"N/A\"\n",
                "    print(f\"  F1: {f1:.4f}, ROC-AUC: {roc_str}, CV: {cv_scores.mean():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèÜ Step 9: Select Best Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_name = max(results, key=lambda x: results[x]['f1_weighted'])\n",
                "best_model = results[best_name]['model']\n",
                "best_score = results[best_name]['f1_weighted']\n",
                "\n",
                "print(f\"\\nüèÜ BEST: {best_name} (F1={best_score:.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìà Step 10: Confusion Matrices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
                "for idx, (name, m) in enumerate(results.items()):\n",
                "    cm = confusion_matrix(y_test, m['y_pred'])\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes.flat[idx],\n",
                "                xticklabels=CLASS_LABELS.values(), yticklabels=CLASS_LABELS.values())\n",
                "    axes.flat[idx].set_title(f\"{name}\\nF1={m['f1_weighted']:.4f}\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('confusion_matrices.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üíæ Step 11: Save Model, Scaler, and Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model\n",
                "joblib.dump(best_model, 'psychiatric_model.joblib')\n",
                "print(\"‚úÖ psychiatric_model.joblib\")\n",
                "\n",
                "# Save scaler (IMPORTANT for inference)\n",
                "joblib.dump(scaler, 'scaler.joblib')\n",
                "print(\"‚úÖ scaler.joblib\")\n",
                "\n",
                "# Save feature names\n",
                "with open('feature_names.json', 'w') as f:\n",
                "    json.dump(available_features, f)\n",
                "print(\"‚úÖ feature_names.json\")\n",
                "\n",
                "# Save training report\n",
                "report = {'best_model': best_name, 'models': {}}\n",
                "for name, m in results.items():\n",
                "    report['models'][name] = {\n",
                "        k: float(v) if isinstance(v, (int, float)) and v is not None else v\n",
                "        for k, v in m.items() if k not in ['model', 'y_pred']\n",
                "    }\n",
                "\n",
                "with open('training_report.json', 'w') as f:\n",
                "    json.dump(report, f, indent=2)\n",
                "print(\"‚úÖ training_report.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì§ Step 12A: Save to Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "\n",
                "if 'DRIVE_OUTPUT' not in globals():\n",
                "    raise RuntimeError(\"Run Step 0 first, or use Step 12B\")\n",
                "\n",
                "for f in ['psychiatric_model.joblib', 'scaler.joblib', 'feature_names.json', \n",
                "          'training_report.json', 'confusion_matrices.png']:\n",
                "    shutil.copy(f, f'{DRIVE_OUTPUT}/{f}')\n",
                "\n",
                "print(f\"‚úÖ Saved to {DRIVE_OUTPUT}\")\n",
                "print(\"\\nüìÅ Copy to backend/models/:\")\n",
                "print(\"   - psychiatric_model.joblib\")\n",
                "print(\"   - scaler.joblib\")\n",
                "print(\"   - feature_names.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì• Step 12B: Download Files (Browser Colab)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "for f in ['psychiatric_model.joblib', 'scaler.joblib', 'feature_names.json', \n",
                "          'training_report.json', 'confusion_matrices.png']:\n",
                "    files.download(f)\n",
                "\n",
                "print(\"\\n‚úÖ Downloaded! Place in backend/models/:\")\n",
                "print(\"   - psychiatric_model.joblib\")\n",
                "print(\"   - scaler.joblib\") \n",
                "print(\"   - feature_names.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß™ Step 13: Test Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with sample input\n",
                "sample = [[2] * len(available_features)]\n",
                "sample_scaled = scaler.transform(sample)\n",
                "\n",
                "pred = best_model.predict(sample_scaled)[0]\n",
                "probs = best_model.predict_proba(sample_scaled)[0]\n",
                "\n",
                "print(f\"Sample (all 2s) ‚Üí {CLASS_LABELS[pred]} ({max(probs)*100:.1f}% confidence)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ‚úÖ Done!\n",
                "\n",
                "**Files to copy to `backend/models/`:**\n",
                "1. `psychiatric_model.joblib`\n",
                "2. `scaler.joblib`\n",
                "3. `feature_names.json`\n",
                "\n",
                "The backend will automatically load the scaler and apply it before predictions."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}